# 统计学习方法(第2版)-第02章总结

## 1. 感知机模型

- 适用于**线性二分类**任务。

- 假设输入空间（特征空间）是$\mathcal{X} \subseteq \mathbf{R}^{n}$，输出空间是$\mathcal{Y}=\{+1,-1\}$

- 输入空间$x \in \mathcal{X}$表示样本实例的特征向量，输出 $y \in \mathcal{Y}$ 表示样本实例的类别（即标签，如0，1），两者一一对应。

- **感知机**：由输入空间到输出空间的映射函数：
  $$
  f(x) = \operatorname{sign}(w \cdot x+b)
  $$

- $w$表示**权重向量**，$b$表示**偏置项**，$x$表示**特征向量**

- **符号函数**：
  $$
  \operatorname{sign}(x)=\left\{\begin{array}{ll}
  +1, & x \geqslant 0 \\
  -1, & x<0
  \end{array}\right.
  $$

##  2. 感知机模型原理分析

- **几何解释：**对于线性方程，
  $$
  w \cdot x+b=0
  $$

- 对应于超平面$s$,  $w$为法向量,  $b$截距分离正、负类
- 分离超平面，如下：

![image-20210226185901538](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210226185901538.png)

##  3. 感知机学习策略

-  如何定义损失函数？ 

-  **自然选择**：误分类点的数目，但损失函数不是$w, b $连续可导，不宜优化。 

-  **另一选择**：误分类点到超平面的总距离： 

- 距离：
  $$
  \frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
  $$

- 误分类点：
  $$
  -y_{i}\left(w \cdot x_{i}+b\right)>0
  $$

- 误分类点距离：
  $$
  -\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)
  $$

- 总距离：    
  $$
  -\frac{1}{\|w\|} \sum_{x_{i}\in M} y_{i}\left(w \cdot x_{i}+b\right)
  $$

- **损失函数**：$M$为误分类点的数目 
  $$
  L(w, b)=-\sum_{x_{i}, \in M} y_{i}\left(w \cdot x_{i}+b\right)
  $$

## 4. 感知机学习算法

-  求解最优化问题： 
  $$
  L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
  $$

-  **随机梯度下降法，** 首先任意选择一个超平面，$w，b$然后不断极小化目标函数，损失函数$L$的梯度： 
  $$
  \begin{array}{c}
  \nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i} \\
  \nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}
  \end{array}
  $$

-  选取误分类点更新： 
  $$
  \begin{aligned}
  w & \leftarrow w+\eta y_{i} x_{i} \\
  & b \leftarrow b+\eta y_{i}
  \end{aligned}
  $$

$$
\eta(0<\eta \leqslant 1) \text { 是步长，在统计学习中又称为学习率（learning rate） }
$$

- **算法流程的原始形式：**

  ![image-20210226204427465](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210226204427465.png)

-  **定理表明：** 

-  误分类的次数k是有上界的，当训练数据集线性可 分时，感知机学习算法原始形式迭代是收敛的。 

  ![image-20210227001335478](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227001335478.png)

-  感知机算法存在许多解，既依赖于初值，也依赖 迭代过程中误分类点的选择顺序。

- 为得到唯一分离超平面，需要增加约束，如```SVM```。 

- 线性不可分数据集，迭代震荡。 

- **算法收敛性的证明**：

  $1)$ 由于训练数据集是线性可分的，按照已知定理 ，假设存在超平面可将训练数据集完全正确分开，取此超平面为 $\hat{w}_{\text {opt }} \cdot \hat{x}=w_{\text {opt }} \cdot x+b_{\text {opt }}=0,$ 使 $\left\|\hat{w}_{\text {ont }}\right\|=1$ 。由 于对有限的 $i=1,2, \cdots, N,$ 均有
  $$
  y_{i}\left(\hat{w}_{\text {opt }} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\text {opt }} \cdot x_{i}+b_{\text {opt }}\right)>0 \tag 1
  $$
  所以存在
  $$
  \gamma=\min _{i}\left\{y_{i}\left(w_{\text {opt }} \cdot x_{i}+b_{\text {opt }}\right)\right\} \tag 2
  $$
  使
  $$
  y_{i}\left(\hat{w}_{\text {opt }} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\text {opt }} \cdot x_{i}+b_{\text {opt }}\right) \geqslant \gamma \tag 3
  $$
  $2) $感知机算法从 $\hat{w}_{0}=0$ 开始，如果实例被误分类，则更新权重。令 $\hat{w}_{k-1}$ 是第$k$ 个误分类实例之前的扩充权重向量，即
  $$
  \hat{w}_{k-1}=\left(w_{k-1}^{\mathrm{T}}, b_{k-1}\right)^{\mathrm{T}} \tag 4
  $$
  则第 $k$ 个误分类实例的条件是
  $$
  y_{i}\left(\hat{w}_{k-1} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{k-1} \cdot x_{i}+b_{k-1}\right) \leqslant 0 \tag 5
  $$
  若 $\left(x_{i}, y_{i}\right)$ 是被 $\hat{w}_{k-1}=\left(w_{k-1}^{\mathrm{T}}, b_{k-1}\right)^{\mathrm{T}}$ 误分类的数据，则 $w$ 和 $b$ 的更新是
  $$
  \begin{array}{l}
  w_{k} \leftarrow w_{k-1}+\eta y_{i} x_{i} \\
  b_{k} \leftarrow b_{k-1}+\eta y_{i}
  \end{array}
  $$
  即
  $$
  \hat{w}_{k}=\hat{w}_{k-1}+\eta y_{i} \hat{x}_{i} \tag 6
  $$
  下面开始推导不等式$(7)$

$$
\hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \geqslant k \eta \gamma \tag 7
$$

- 根据公式$(1)、(6)$可得，
  $$
  \begin{aligned}
  \hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} &=\hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta y_{i} \hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i} \\
  & \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma
  \end{aligned} \tag 8
  $$
  由此递推即得不等式 $(7)$
  $$
  \begin{array}{c}
  \hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma \geqslant \hat{w}_{k-2} \cdot \hat{w}_{\mathrm{opt}}+2 \eta \gamma \geqslant \cdots \geqslant k \eta \gamma \\
  \left\|\hat{w}_{k}\right\|^{2} \leqslant k \eta^{2} R^{2}
  \end{array} \tag 9
  $$
  由公式 $(3) 、(6) $得
  $$
  \begin{aligned}
  \left\|\hat{w}_{k}\right\|^{2} &=\left\|\hat{w}_{k-1}\right\|^{2}+2 \eta y_{i} \hat{w}_{k-1} \cdot \hat{x}_{i}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
  & \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
  & \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2} R^{2} \\
  & \leqslant\left\|\hat{w}_{k-2}\right\|^{2}+2 \eta^{2} R^{2} \leqslant \cdots \\
  & \leqslant k \eta^{2} R^{2}
  \end{aligned}
  $$
  结合不等式$ (5) 、(6)$ 得
  $$
  \begin{array}{l}
  k \eta \gamma \leqslant \hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \leqslant\left\|\hat{w}_{k}\right\|\left\|\hat{w}_{\mathrm{opt}}\right\| \leqslant \sqrt{k} \eta R \\
  k^{2} \gamma^{2} \leqslant k R^{2}
  \end{array}
  $$
  最终可证明成立
  $$
  k \leqslant\left(\frac{R}{\gamma}\right)^{2}
  $$
  

- **算法流程的对偶形式**：

  ![image-20210227000444920](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227000444920.png)

![image-20210227000510576](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227000510576.png)

- **对偶形式的基本想法：**
-  将$w$和$b$表示为实例$x_{i}$和标记$y_{i}$的线性组会的形式， 通过求解其系数而求得$w$和$b$，对误分类点： 

$$
\begin{array}{c}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
b=\sum_{i=1}^{N} \alpha_{i} y_{i}
\end{array}
$$

- 这里, $\alpha_{i} \geqslant 0, i=1,2, \cdots, N,$ 当 $\eta=1$ 时，表示第 $i$ 个实例点由于误分而进行更新的 次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。

## 5. 代码示例

- Github地址： [第02章 感知机](https://github.com/zjxi/ML-Algorithms/tree/main/%E7%AC%AC02%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA)