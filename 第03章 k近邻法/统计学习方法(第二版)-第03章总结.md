## 统计学习方法(第二版)-第03章总结

## 1. K近邻模型

- **优点：**
  - 精度高
  - 对异常值不敏感
  - 无数据输入假定

- **缺点：**
  - 计算复杂度高
  - 空间复杂度高

- **适用数据范围**
  - 数值型、标称型

- **模型三要素：**
  - $k$值的选择
  - 距离度量
  - 分类决策规则

- **工作原理：**
  - 存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。
  - 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。
  - 一般来说，只选择样本数据集中前N个最相似的数据。$K$一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类

## 2.  K近邻算法的一般流程

- 收集数据：可以使用任何方法。
- 准备数据：距离计算所需要的数值，最后是结构化的数据格式。
- 分析数据：可以使用任何方法。
- 训练算法：此步骤KNN中不适用。
- 测试算法：计算错误率。
- 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行K近邻算法判定输入数据分别属于哪个分类，最后应用对计算 出的分类执行后续的处理。 

## 3. 距离度量

- 特征向量：
  $$
  x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}
  $$

- $LP$距离：
  $$
  L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(i)}\right|^{p}\right)^{\frac{1}{p}}
  $$

![image-20210227153913347](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227153913347.png)

- 这里$p \geqslant 1$，当$p=2$时，称为**欧式距离**；

- 当$p=1$时，称为**曼哈顿距离**；

- 当$p=\infty$时，即为各个坐标系的最大值，即
  $$
  L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
  $$

## 4. K值的选择

-  如果选择**较小**的K值： 
  - “学习”的**近似误差**会减小，但 “学习”的**估计误差**会增大。
  -  噪声敏感。
  - K值的减小就意味着整体模型变得复杂，易发生过拟合。

-  如果选择**较大**的K值：
  - 减少学习的估计误差，但缺点是学习的近似误差会增大。
  - K值的增大 就意味着整体的模型变得简单.。

## 5. 分类决策规则

- 往往采用**多数表决**，即由输入实例中的$k$个近邻的训练实例中的多数类别决定。
- 多数表决规则等价于经验风险最小化。

## 6. K近邻的算法流程：

![image-20210227164750902](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227164750902.png)

![image-20210227164806654](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227164806654.png)

## 7. KD树实现

- **构造KD树：**

  - K近邻最容易的实现方法是线性扫描，但当训练集数量非常大时，计算时间成本高，不可行。

  - KD树是一种对K维空间中的实例点进行存储以便对其进行快速检 索的树形数据结构。

  - KD树是二叉树，表示对K维空间的一个划分，构造KD树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一 系列的k维超矩形区域.Kd树的每个结点对应于一个$k$维超矩形区域。

    ![image-20210227165326858](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227165326858.png)

- **搜索KD树：**

  ![image-20210227165412608](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20210227165412608.png)

- 若数据集的样本点是**随机分布**的，kd树的平均搜索计算复杂度为$O(logN)$，$N$即为训练数据集样本数。
- kd树适用于**数据集样本数远大于数据集维度**，若两者数量相近，则计算复杂度无异于线性扫描。
- kd树适用于递归法，实现方便。

## 8. 代码示例

- Github地址：[第03章 k近邻法](https://github.com/zjxi/ML-Algorithms/blob/main/%E7%AC%AC03%E7%AB%A0%20k%E8%BF%91%E9%82%BB%E6%B3%95/knn.py)

